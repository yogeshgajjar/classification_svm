{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-CLASS AND MULTI-LABEL CLASSIFICATION USING SVM\n",
    "\n",
    "### Part (a) - To download the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiliazation of libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniatialization of libraries \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')  # Used to avoid any warnings in the output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pandas Dataframe from the Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Frogs_MFCCs.csv', usecols = range(1,26))\n",
    "df= pd.DataFrame(data)\n",
    "X = df.iloc[:, 0:21]\n",
    "y = df.iloc[:, 21:24]\n",
    "\n",
    "# Encoding the class in each label i.e. allocating int values to string values of class in each label\n",
    "le = LabelEncoder()\n",
    "y_f= le.fit_transform(y.iloc[:,0])\n",
    "y_g = le.fit_transform(y.iloc[:,1])\n",
    "y_s = le.fit_transform(y.iloc[:,2])\n",
    "\n",
    "# Making a new dataframe after LabelEncoder function. \n",
    "y_new = pd.DataFrame({'Family': y_f, 'Genus': y_g, 'Species': y_s})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)ii  - To train model for each label using SVM RBF kernel. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty and width of gaussian kernel(gamma) is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100 and the gamma varies from 0.05 to 2. \n",
    "\n",
    "### We use OneVsRest classifier to train SVM for each of the label. \n",
    "\n",
    "### The label classified here is \"FAMILY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING SVM WITH RBF KERNEL\n",
      "LABEL - FAMILY\n",
      "\n",
      "Test Score is : 0.9944418712366836\n",
      "Hamming loss is : 0.00555812876331635\n",
      "Best Penalty is: 10.29387755102041 and Best Gamma is : 1.8806122448979592\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100), 'gamma': np.linspace(0.05,2)}\n",
    "#parameters = {'C': np.linspace(0.1, 100, 100), 'gamma': np.linspace(0.05,0.1,2)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(SVC(kernel='rbf'), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Family'])\n",
    "\n",
    "#Instanciating of the classifier OneVsRest to train SVC with kernel RBF. \n",
    "estimator = SVC(C = gs.best_params_.get('C'), kernel = 'rbf', gamma = gs.best_params_.get('gamma'))\n",
    "ovr = OneVsRestClassifier(estimator)\n",
    "ovr.fit(X_train, y_train.loc[:,'Family'])\n",
    "y_pred_f = ovr.predict(X_test)   # Calculates the y_pred value. \n",
    "loss_f = hamming_loss(y_test.loc[:,'Family'], y_pred_f)  #The indiviadual hamming loss is calculated. \n",
    "\n",
    "print(\"CLASSIFICATION USING SVM WITH RBF KERNEL\")\n",
    "print(\"LABEL - FAMILY\")\n",
    "print(\"\")\n",
    "print(\"Test Score is :\", ovr.score(X_test, y_test.loc[:,'Family'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_f)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'), \"and Best Gamma is :\", gs.best_params_.get('gamma'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion(for FAMILY label) - We see that the test accuracy score is 0.99 which is expectionally good. Almost total classification. The classifier is working properly for the unseen test data. The best penalty is aroud 10.29 and best gamma is 1.88 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)ii  - To train model for each label using SVM RBF kernel. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty and width of gaussian kernel(gamma) is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100 and the gamma varies from 0.05 to 2. \n",
    "\n",
    "### We use OneVsRest classifier to train SVM for each of the label. \n",
    "\n",
    "### The label classified here is \"GENUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING SVM WITH RBF KERNEL\n",
      "LABEL - GENUS\n",
      "\n",
      "Test Score is : 0.9898100972672533\n",
      "Hamming loss is : 0.010189902732746642\n",
      "Best Penalty is: 12.332653061224491 and Best Gamma is : 1.920408163265306\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100), 'gamma': np.linspace(0.05,2)}\n",
    "#parameters = {'C': np.linspace(0.1, 100, 100), 'gamma': np.linspace(0.05,0.1,2)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(SVC(kernel='rbf'), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Genus'])\n",
    "\n",
    "#Instanciating of the classifier OneVsRest to train SVC with kernel RBF. \n",
    "estimator = SVC(C = gs.best_params_.get('C'), kernel = 'rbf', gamma = gs.best_params_.get('gamma'))\n",
    "ovr = OneVsRestClassifier(estimator)\n",
    "ovr.fit(X_train, y_train.loc[:,'Genus'])\n",
    "y_pred_g = ovr.predict(X_test)     # Calculates the y_pred value. \n",
    "loss_g = hamming_loss(y_test.loc[:,'Genus'], y_pred_g)     #The indiviadual hamming loss is calculated. \n",
    "\n",
    "print(\"CLASSIFICATION USING SVM WITH RBF KERNEL\")\n",
    "print(\"LABEL - GENUS\")\n",
    "print(\"\")\n",
    "print(\"Test Score is :\", ovr.score(X_test, y_test.loc[:,'Genus'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_g)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'), \"and Best Gamma is :\", gs.best_params_.get('gamma'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - GENUS) - We see that the test accuracy score is 0.98 which is expectionally good. Almost total classification. The classifier is working properly for the unseen test data. The best penalty is aroud 12.33 and best gamma is 1.92 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)ii  - To train model for each label using SVM RBF kernel. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty and width of gaussian kernel(gamma) is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100 and the gamma varies from 0.05 to 2. \n",
    "\n",
    "### We use OneVsRest classifier to train SVM for each of the label. \n",
    "\n",
    "### The label classified here is \"SPECIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING SVM WITH RBF KERNEL\n",
      "LABEL - SPECIES\n",
      "\n",
      "Test Score is : 0.9893469198703103\n",
      "Hamming loss is : 0.010653080129689671\n",
      "Best Penalty is: 6.216326530612245 and Best Gamma is : 0.9255102040816326\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100), 'gamma': np.linspace(0.05,2)}\n",
    "#parameters = {'C': np.linspace(0.1, 100, 100), 'gamma': np.linspace(0.05,0.1,2)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(SVC(kernel='rbf'), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Species'])\n",
    "\n",
    "#Instanciating of the classifier OneVsRest to train SVC with kernel RBF. \n",
    "estimator = SVC(C = gs.best_params_.get('C'), kernel = 'rbf', gamma = gs.best_params_.get('gamma'))\n",
    "ovr = OneVsRestClassifier(estimator)\n",
    "ovr.fit(X_train, y_train.loc[:,'Species'])\n",
    "y_pred_s = ovr.predict(X_test)\n",
    "loss_s = hamming_loss(y_test.loc[:,'Species'], y_pred_s)\n",
    "\n",
    "print(\"CLASSIFICATION USING SVM WITH RBF KERNEL\")\n",
    "print(\"LABEL - SPECIES\")\n",
    "print(\"\")\n",
    "print(\"Test Score is :\", ovr.score(X_test, y_test.loc[:,'Species'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_s)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'), \"and Best Gamma is :\", gs.best_params_.get('gamma'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - SPECIES) - We see that the test accuracy score is 0.989 which is expectionally good. Almost total classification. The classifier is working properly for the unseen test data. The best penalty is aroud 6.216 and best gamma is 0.9255 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXACT MATCH LOSS AND HAMMING LOSS FOR RBF KEREL\n",
    "\n",
    "### Exact match loss is basically calculated by comparing the y_true and y_pred. If all 3 classifier predict correctly we count it as 1 and if one of them misclasssifies, we count it as 0. \n",
    "\n",
    "### Hamming loss is calculated by taking the average of all individual hamming loss. Here we have 3 labels, so we divide the total hamming loss by 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXACT MATCH LOSS AND HAMMING LOSS FOR RBF KERNEL\n",
      "\n",
      "Exact Match loss : 0.014358499305233918\n",
      "Total Hamming Loss : 0.008800370541917554\n"
     ]
    }
   ],
   "source": [
    "rbf_loss = 0\n",
    "\n",
    "# Loop for comparing the y_true and y_pred. If they match, we increament. \n",
    "for i in range(len(X_test)):\n",
    "    if y_test.loc[:,'Family'].values[i] == y_pred_f[i] and y_test.loc[:,'Genus'].values[i] == y_pred_g[i] and  y_test.loc[:,'Species'].values[i] == y_pred_s[i]:\n",
    "        rbf_loss = rbf_loss + 1\n",
    "    else:\n",
    "        rbf_loss = rbf_loss \n",
    "        \n",
    "emloss_rbf = rbf_loss/len(X_test)   # The total is divied by total no of X_test data. \n",
    "hloss_rbf = (loss_f + loss_g + loss_s)/3   # Total Hamming loss by taking average of all the three classifiers\n",
    "\n",
    "print(\"EXACT MATCH LOSS AND HAMMING LOSS FOR RBF KERNEL\")\n",
    "print(\"\")\n",
    "print(\"Exact Match loss :\", 1 - emloss_rbf)\n",
    "print(\"Total Hamming Loss :\", hloss_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION - We see that the total hamming loss is less than the exact match loss and which is ideally the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion about the SVC model with RBF Kernel -- We saw that in every label we got perfect fit for the model. The accuracy turned out to be almost 99% and the parameters for gamma remained between 0.9 to 1.8. This implies that we got a good seperability among classes in each label using RBF kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iii  - To train model for each label using SVM linear kernel and L1 penalized. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"FAMILY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "LABEL - FAMILY\n",
      "\n",
      "Test Score is : 0.9249652616952293\n",
      "Hamming loss is : 0.07503473830477073\n",
      "Best Penalty is: 14.371428571428572\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Family'])\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train, y_train.loc[:,'Family'])\n",
    "y_pred_f_l1 = svc.predict(X_test)\n",
    "loss_f_l1 = hamming_loss(y_test.loc[:,'Family'], y_pred_f_l1)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\")\n",
    "print(\"LABEL - FAMILY\")\n",
    "print(\"\")\n",
    "print(\"Test Score is :\", svc.score(X_test, y_test.loc[:,'Family'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_f_l1)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - FAMILY) - We see that the test accuracy score is 0.92 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 14.37 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iii  - To train model for each label using SVM linear kernel and L1 penalized. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"GENUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "LABEL - GENUS\n",
      "\n",
      "Test score is : 0.9407132931912923\n",
      "Hamming loss is : 0.059286706808707734\n",
      "Best Penalty is: 97.96122448979592\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Genus'])\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train, y_train.loc[:,'Genus'])\n",
    "y_pred_g_l1 = svc.predict(X_test)\n",
    "loss_g_l1 = hamming_loss(y_test.loc[:,'Genus'], y_pred_g_l1)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\")\n",
    "print(\"LABEL - GENUS\")\n",
    "print(\"\")\n",
    "print(\"Test score is :\", svc.score(X_test, y_test.loc[:,'Genus'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_g_l1)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - GENUS) - We see that the test accuracy score is 0.94 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 97.96 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iii  - To train model for each label using SVM linear kernel and L1 penalized. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"SPECIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\n",
      "LABEL - SPECIES\n",
      "\n",
      "Test score is : 0.9592403890690134\n",
      "Hamming loss is : 0.04075961093098657\n",
      "Best Penalty is: 61.263265306122456\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train, y_train.loc[:,'Species'])\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train, y_train.loc[:,'Species'])\n",
    "y_pred_s_l1 = svc.predict(X_test)\n",
    "loss_s_l1 = hamming_loss(y_test.loc[:,'Species'], y_pred_s_l1)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL\")\n",
    "print(\"LABEL - SPECIES\")\n",
    "print(\"\")\n",
    "print(\"Test score is :\", svc.score(X_test, y_test.loc[:,'Species'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_s_l1)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - SPECIES) - We see that the test accuracy score is 0.95 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 61.26 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED \n",
    "\n",
    "### Exact match loss is basically calculated by comparing the y_true and y_pred. If all 3 classifier predict correctly we count it as 1 and if one of them misclasssifies, we count it as 0. \n",
    "\n",
    "### Hamming loss is calculated by taking the average of all individual hamming loss. Here we have 3 labels, so we divide the total hamming loss by 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED\n",
      "\n",
      "Exact Match loss : 0.09078276980083377\n",
      "Total Hamming Loss : 0.058360352014821676\n"
     ]
    }
   ],
   "source": [
    "l1_loss = 0\n",
    "\n",
    "# Loop for comparing the y_true and y_pred. If they match, we increament. \n",
    "for i in range(len(X_test)):\n",
    "    if y_test.loc[:,'Family'].values[i] == y_pred_f_l1[i] and y_test.loc[:,'Genus'].values[i] == y_pred_g_l1[i] and y_test.loc[:,'Species'].values[i] == y_pred_s_l1[i]:\n",
    "        l1_loss = l1_loss + 1\n",
    "    else:\n",
    "        l1_loss = l1_loss \n",
    "emloss_l1 = l1_loss/len(X_test)   # The total is divied by total no of X_test data.\n",
    "hloss_l1 = (loss_f_l1 + loss_g_l1 + loss_s_l1)/3   # Total Hamming loss by taking average of all the three classifiers\n",
    "\n",
    "print(\"EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED\")\n",
    "print(\"\")\n",
    "print(\"Exact Match loss :\", 1 - emloss_l1)\n",
    "print(\"Total Hamming Loss :\", hloss_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION - We see that the total hamming loss is less than the exact match loss and which is ideally the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion - \n",
    "### We see that with with linear kernel and L1 regularization, the error increased and the accuracy decreased when compared to the RBF kernel. We know that L1 penalty provides sparse and feature elimination. In this case we have large number of datapoints and when L1 penalty eliminates feature, it increases error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iv  - To train model for each label using SVM linear kernel and L1 penalized and SMOTE for class imbalance. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"FAMILY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state= 10)\n",
    "\n",
    "# Using SMOTE to deal with class imbalance. Creating individual dataframe and fitting it. \n",
    "# This is for training data. \n",
    "X_train_f_smote, y_train_f_smote = sm.fit_resample(X_train, y_train.loc[:,'Family'])\n",
    "X_train_g_smote, y_train_g_smote = sm.fit_resample(X_train, y_train.loc[:,'Genus'])\n",
    "X_train_s_smote, y_train_s_smote = sm.fit_resample(X_train, y_train.loc[:,'Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\n",
      "LABEL - FAMILY\n",
      "\n",
      "Test Score is : 0.9092172301991662\n",
      "Hamming loss is : 0.09078276980083372\n",
      "Best penalty is: 97.96122448979592\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train_f_smote, y_train_f_smote)\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train_f_smote, y_train_f_smote)\n",
    "y_pred_f_l1_smote = svc.predict(X_test)\n",
    "loss_f_l1_smote = hamming_loss(y_test.loc[:,'Family'], y_pred_f_l1_smote)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\")\n",
    "print(\"LABEL - FAMILY\")\n",
    "print(\"\")\n",
    "print(\"Test Score is :\", svc.score(X_test, y_test.loc[:,'Family'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_f_l1_smote)\n",
    "print(\"Best penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - FAMILY) - We see that the test accuracy score is 0.90 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 97.96 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iv  - To train model for each label using SVM linear kernel and L1 penalized and SMOTE for class imbalance. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"GENUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\n",
      "LABEL - GENUS\n",
      "\n",
      "Test score is : 0.9069013432144511\n",
      "Hamming loss is : 0.09309865678554886\n",
      "Best Penalty is: 79.61224489795919\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train_g_smote, y_train_g_smote)\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train_g_smote, y_train_g_smote)\n",
    "y_pred_g_l1_smote = svc.predict(X_test)\n",
    "loss_g_l1_smote = hamming_loss(y_test.loc[:,'Genus'], y_pred_g_l1_smote)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\")\n",
    "print(\"LABEL - GENUS\")\n",
    "print(\"\")\n",
    "print(\"Test score is :\", svc.score(X_test, y_test.loc[:,\"Genus\"], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_g_l1_smote)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - GENUS) - We see that the test accuracy score is 0.90 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 79.61 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)iv  - To train model for each label using SVM linear kernel and L1 penalized and SMOTE for class imbalance. \n",
    "\n",
    "### Here, the parameters i.e. weight of SVM penalty is selected using gridsearchCV with 10 fold CV. The SVM penalty varies from 0.1 to 100.  \n",
    "\n",
    "### We use LinearSVC classifier to train SVM for each of the label. The loss is sqaured-hinge. \n",
    "\n",
    "### The label classified here is \"SPECIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\n",
      "LABEL - SPECIES\n",
      "\n",
      "Test score is : 0.9587772116720704\n",
      "Hamming loss is : 0.041222788327929596\n",
      "Best Penalty is: 69.41836734693878\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.1, 100)}\n",
    "\n",
    "# GridsearchCV used to identify the best parameters from the range declared above. \n",
    "gs = GridSearchCV(LinearSVC(), parameters, cv = 10)\n",
    "gs.fit(X_train_s_smote, y_train_s_smote)\n",
    "\n",
    "#Instanciating of the classifier LinearSVC to train SVC with kernel Linear and l1-penalized with loss = squared hinge loss. \n",
    "svc = LinearSVC(penalty='l1', loss='squared_hinge', dual = False, C = gs.best_params_.get('C'), multi_class='ovr')\n",
    "svc.fit(X_train_s_smote, y_train_s_smote)\n",
    "y_pred_s_l1_smote = svc.predict(X_test)\n",
    "loss_s_l1_smote = hamming_loss(y_test.loc[:,'Species'], y_pred_s_l1_smote)\n",
    "\n",
    "print(\"CLASSIFICATION USING L1 PENALIZED SVM WITH LINEAR KERNEL(SMOTE)\")\n",
    "print(\"LABEL - SPECIES\")\n",
    "print(\"\")\n",
    "print(\"Test score is :\", svc.score(X_test, y_test.loc[:,'Species'], sample_weight=None))\n",
    "print(\"Hamming loss is :\", loss_s_l1_smote)\n",
    "print(\"Best Penalty is:\", gs.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION(For label - SPECIES) - We see that the test accuracy score is 0.95 which is good. Prone to missclassification. The classifier is working properly for the unseen test data. The best penalty is aroud 69.41 as calculated by the gridsearchCV and as a result be got a very good accuracy in test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED WITH SMOTE\n",
    "\n",
    "### Exact match loss is basically calculated by comparing the y_true and y_pred. If all 3 classifier predict correctly we count it as 1 and if one of them misclasssifies, we count it as 0. \n",
    "\n",
    "### Hamming loss is calculated by taking the average of all individual hamming loss. Here we have 3 labels, so we divide the total hamming loss by 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED WITH SMOTE\n",
      "\n",
      "Exact Match loss : 0.14126910606762388\n",
      "Total Hamming Loss : 0.07503473830477071\n"
     ]
    }
   ],
   "source": [
    "smote_loss = 0\n",
    "\n",
    "# Loop for comparing the y_true and y_pred. If they match, we increament. \n",
    "for i in range(len(X_test)):\n",
    "    if y_test.loc[:,'Family'].values[i] == y_pred_f_l1_smote[i] and y_test.loc[:,'Genus'].values[i] == y_pred_g_l1_smote[i] and y_test.loc[:,'Species'].values[i] == y_pred_s_l1_smote[i]:\n",
    "        smote_loss = smote_loss + 1\n",
    "    else:\n",
    "        smote_loss = smote_loss \n",
    "emloss_smote = smote_loss/len(X_test)   # The total is divied by total no of X_test data.\n",
    "hloss_smote = (loss_f_l1_smote + loss_g_l1_smote + loss_s_l1_smote)/3     # Total Hamming loss by taking average of all the three classifiers\n",
    "\n",
    "print(\"EXACT MATCH LOSS AND HAMMING LOSS FOR L1 PENALIZED WITH SMOTE\")\n",
    "print(\"\")\n",
    "print(\"Exact Match loss :\", 1- emloss_smote)\n",
    "print(\"Total Hamming Loss :\", hloss_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION - We see that the total hamming loss is less than the exact match loss and which is ideally the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion - \n",
    "### To match the class imbalance, we used SMOTE on the training data. We see that the error got increased than the normal L1 penalized SVM. This is probably because SMOTE increased misclassfication by increasing datapoints. Since we are using the same L1 penalized SVM model, it eliminated features on such huge dataset which in turn made the model more prone to missclassification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
